{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "c479d491bf3d7518f8b0ced78e6bfc684cfc1cc4534854408cc51ac76a6a9bee"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "__author__ = \"Alina Molnar\"\r\n",
    "__copyright__ = \"Copyright (C) 2020-2021 Alina Molnar\"\r\n",
    "__license__ = \"CC BY-NC\"\r\n",
    "__version__ = \"1.0\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 1. IMPORT LIBRARIES"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import glob\r\n",
    "import os\r\n",
    "from math import ceil\r\n",
    "from pathlib import PureWindowsPath\r\n",
    "\r\n",
    "import h2o\r\n",
    "import IPython\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "from h2o.estimators import (H2OGradientBoostingEstimator,\r\n",
    "                            H2ORandomForestEstimator)\r\n",
    "from IPython.display import display\r\n",
    "from scipy.stats import spearmanr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Settings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set terminal to show all columns because I am interested in having an overview, not just the first few and the last few.\r\n",
    "pd.options.display.max_rows = 99\r\n",
    "\r\n",
    "# Do the same to show all rows because dataset is small.\r\n",
    "pd.options.display.max_columns = 99\r\n",
    "\r\n",
    "# Setting the colorpalette to \"Colorblind\" creates graphs accesible to everyone by removing red and green.\r\n",
    "sns.set_palette(\"colorblind\")\r\n",
    "\r\n",
    "# Show info about system to help others reproduce the code.\r\n",
    "print(IPython.sys_info())\r\n",
    "\r\n",
    "plt.rcParams['figure.dpi'] = 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 2. DATA UNDERSTANDING. CLEAN, TRANSFORM, PREPROCESS DATA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Collect Initial Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build the path to the following file in the repo: \\beer_input\\beer_241.xlsx\r\n",
    "input_file = os.getcwd() + os.sep + \"beer_input\" + os.sep + \"beer_241.xlsx\"\r\n",
    "\r\n",
    "# Read beer file\r\n",
    "beer_all = pd.read_excel(input_file, sheet_name=\"Sheet1\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Describe Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write function that takes any dataframe and displays basic details about it.\r\n",
    "# Use display() because it automatically adds an empty line between outputs, instead of print() which doesn't.\r\n",
    "\r\n",
    "def show_basic_stats(df):\r\n",
    "    \"\"\"Display dataframe properties.\"\"\"\r\n",
    "    \r\n",
    "    display(df.shape)\r\n",
    "    display(df.info())\r\n",
    "    display(df.describe().round(1))\r\n",
    "    \r\n",
    "show_basic_stats(beer_all)\r\n",
    "# Rating is between -1 and 11."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.1 Data Preprocessing: Standardization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Standardize appeareance. Convert column labels to lowercase.\r\n",
    "beer_all.columns = beer_all.columns.str.lower()\r\n",
    "\r\n",
    "# Convert columns values to lowercase if they are strings.\r\n",
    "beer_all = beer_all.applymap(lambda col:col.lower() if type(col) == str else col)\r\n",
    "\r\n",
    "# Convert Name column from object to string.\r\n",
    "beer_all[\"name\"] = beer_all[\"name\"].astype(\"string\")\r\n",
    "\r\n",
    "# Cut alcohol content from end of name and store as separate column.\r\n",
    "beer_all[\"abv\"] = [name.rsplit(maxsplit=1)[-1] for name in beer_all[\"name\"]]\r\n",
    "\r\n",
    "# Convert alcohol content to float.\r\n",
    "beer_all[\"abv\"] = beer_all[\"abv\"].astype(float)\r\n",
    "\r\n",
    "# Convert object types to category, except Split column.\r\n",
    "beer_all[[\"method\", \"style\", \"flavor\", \"fermentation\"]] = beer_all[[\"method\", \"style\", \"flavor\", \"fermentation\"]].astype(\"category\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.2 Data Preprocessing: Pre-Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Validation of uniqueness in beer names. Check for duplicates, remove if found.\r\n",
    "beer_all.drop_duplicates(subset=\"name\", keep=\"last\", inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.3 Data Preprocessing: Numeric Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Description of numeric variables after standardized appearance and after removal of duplicates.\r\n",
    "show_basic_stats(beer_all)\r\n",
    "\r\n",
    "# At the beginning there were 241 rows and 8 columns, now there are 240 rows and 9 columns.\r\n",
    "\r\n",
    "# Percentage of beer with ratings lower than 5.\r\n",
    "under_5_rating = beer_all[\"rating\"] < 5\r\n",
    "under_5_rating_percentage = (len(beer_all[under_5_rating])/len(beer_all[\"rating\"]))*100\r\n",
    "print(f\"Currently {under_5_rating_percentage:.1f}% of total beers are discarded.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.3 Data Preprocessing: Categorization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Description of categorical variables.\r\n",
    "\r\n",
    "def describe_categorical_columns(df, numeric_column):\r\n",
    "    \"\"\"Print basic statistics of subgroups in categorical columns.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "    df (pandas.DataFrame): Dataframe containing numerical and categorical columns.\r\n",
    "    numeric_column (str): Name of the column containing numerical data.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    Print basic statistics of each column containing categorical data: count, mean, std, min, 25%, 50%, 75%, max.\r\n",
    "    \"\"\"\r\n",
    "    # Store in a list the columns containing categorical data.\r\n",
    "    list_categoricals = df.select_dtypes(include=[\"category\"]).columns.tolist()\r\n",
    "    # Iterate through list of categorical columns.\r\n",
    "    for i, elem in enumerate(list_categoricals):\r\n",
    "        description = df.groupby([elem])[numeric_column].describe().round(1)\r\n",
    "        print(description)\r\n",
    "\r\n",
    "describe_categorical_columns(beer_all, \"rating\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.5 Data Preprocessing: String Columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Description of Name column, string type, not categorical.\r\n",
    "name_unique = len(set(beer_all[\"name\"])) \r\n",
    "print(f\"There are {name_unique} unique beer names.\")\r\n",
    "\r\n",
    "# Description of Country column, string type, not categorical.\r\n",
    "country_unique = len(set(beer_all[\"country\"])) \r\n",
    "print(f\"There are {country_unique} unique countries.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Explore Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "### 2.3.1 Hypothesis 1: There might be a linear relationship between ratings and alcohol content."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot alcohol content vs. rating by subgroup to check for hidden patterns.\r\n",
    "\r\n",
    "def scatter_sns(df, x_numeric_column, y_numeric_column):\r\n",
    "    \"\"\"Seaborn scatter type subplots of two numeric variables as x and y, grouped by categorical columns.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "    df (pandas.DataFrame): Dataframe containing numerical and categorical columns.\r\n",
    "    x_numeric_column (str): Name of numerical column to be plotted on x-axis.\r\n",
    "    y_numeric_column (str): Name of numerical column to be plotted on y-axis.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    Seaborn scatter type subplots of x and y series, split by subgroups of categorical columns used as hue.\r\n",
    "    Title and name of axes are added automatically based on the name of x and y series.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # Store in a list the columns containing categorical data.\r\n",
    "    list_categoricals = df.select_dtypes(include=[\"category\"]).columns.tolist()\r\n",
    "\r\n",
    "    # Calculate the number of subplots in the figure.\r\n",
    "    number_of_plots = len(list_categoricals)\r\n",
    "    # Set the number of columns to 3 because it fits most screens.\r\n",
    "    number_of_cols = 3\r\n",
    "    # Calculate the number of rows in which subplots are shown.\r\n",
    "    number_of_rows = ceil(number_of_plots/number_of_cols)\r\n",
    "\r\n",
    "    # Plot figure and set title.\r\n",
    "    fig = plt.figure()\r\n",
    "    fig.suptitle(f\"relationship between {df[x_numeric_column].name} and {df[y_numeric_column].name} by each categorical feature\".title())\r\n",
    "    \r\n",
    "    # Iterate through list of categorical columns.\r\n",
    "    for i, elem in enumerate(list_categoricals):\r\n",
    "        # Add subplots sequentially.\r\n",
    "        # Mark the first subplot as i+1 because subplot indices start at 1, and list indeces start at 0.\r\n",
    "        ax = fig.add_subplot(number_of_rows, number_of_cols, i+1)\r\n",
    "        \r\n",
    "        # Create each subplot, set title of subplot, labels and legend.\r\n",
    "        sns.scatterplot(x=df[x_numeric_column], y=df[y_numeric_column], hue=elem, data=df, s=15)\r\n",
    "        ax.set_title(elem.title(), fontsize=12, verticalalignment=\"bottom\", y=0.97)\r\n",
    "        ax.set(xlabel=df[x_numeric_column].name.capitalize(), ylabel=df[y_numeric_column].name.capitalize())\r\n",
    "        ax.legend(fontsize=5, loc=\"best\")\r\n",
    "        # In VS Code the legend is upper left in minimized window and in best location when maximized.      \r\n",
    "    plt.show()\r\n",
    "\r\n",
    "scatter_sns(beer_all, \"abv\", \"rating\")\r\n",
    "\r\n",
    "# Result 1: The scatterplot shows no linear relationship and no pattern between ratings and ABV.\r\n",
    "# However, subgroup of lemon flavor with zero or low ABV has higher ratings compared to other groups."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.2 Hypothesis 2: Some subgroups might have a low number of observations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create column to store status of occurrences.\r\n",
    "beer_all[\"occurrence\"] = np.nan\r\n",
    "\r\n",
    "# Count occurrences in subgroups of categorical data and return df with subgroups below threshold.\r\n",
    "def select_too_few_categorical_observations(df, categorical_columns, occurrence_column, threshold_percentage):\r\n",
    "    \"\"\"Count observations of categorical features and store result in custom column.\r\n",
    "\r\n",
    "    If subgroup has less observations than threshold, mark them as too_few in a results column.\r\n",
    "\r\n",
    "    Args:\r\n",
    "    df (pandas.DataFrame): Dataframe containing categorical columns.\r\n",
    "    categorical_columns (list): List of categorical columns.\r\n",
    "    occurrence_column (int): Name of column that stores the count of occurrences.\r\n",
    "    threshold_percentage (int, float): Percentage of minimum observations from total.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    df (pandas.DataFrame): Selection from original dataframe.\r\n",
    "    Rows contain subgroups with counted observations less than threshold.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # Calculate the number of rows needed to pass the threshold, and print result.\r\n",
    "    threshold = len(df)*threshold_percentage/100\r\n",
    "    print(f\"The threshold is {threshold} observations or more.\")\r\n",
    "\r\n",
    "    # Iterate through list of categorical columns and count occurrences of subgroups.\r\n",
    "    for i, elem in enumerate(categorical_columns):\r\n",
    "        counted = df[elem].value_counts()\r\n",
    "\r\n",
    "        # Convert counts to dictionary.\r\n",
    "        counted_dictionary = counted.to_dict()\r\n",
    "        # Iterate through dictionary and store result if too_few.\r\n",
    "        for key, value in counted_dictionary.items():\r\n",
    "            if value < threshold:\r\n",
    "                df.loc[df[elem] == key, occurrence_column] = \"too_few\"\r\n",
    "                print(f\"Too few {key} {elem}.\")\r\n",
    "\r\n",
    "    # Fill occurrence column with \"enough\" if the record was not marked as too_few.\r\n",
    "    for elem in df[occurrence_column]:\r\n",
    "        if elem != \"too_few\":\r\n",
    "            df.loc[df[occurrence_column] != \"too_few\", occurrence_column] = \"enough\"\r\n",
    "\r\n",
    "    # Select rows with too_few observations.\r\n",
    "    too_few = df.loc[df[occurrence_column] == \"too_few\"]\r\n",
    "    return too_few\r\n",
    "\r\n",
    "# Define list of categorical features to be checked and set a threshold of 5% from the total.\r\n",
    "categoricals = [\"method\", \"style\", \"flavor\", \"fermentation\"]\r\n",
    "too_few_subgroups = select_too_few_categorical_observations(beer_all, categoricals, \"occurrence\", 5)\r\n",
    "\r\n",
    "# Result 2: Style and Flavor columns have subgroups below the 5% threshold of the total observations."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explain sequence for sorting bars of categoricals by value counts (numeric values) in the following function. Use subgroups of flavor column as example."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start grouping all rows by flavor and select only the rating column, then calculate its mean.\r\n",
    "# Reset the index to avoid future errors.\r\n",
    "grouped_flavor = beer_all.groupby(\"flavor\")[\"rating\"].mean().reset_index()\r\n",
    "# Then sort this series by ratings in descending order and select flavor labels.\r\n",
    "ordered_flavor_mean = grouped_flavor.sort_values(by=\"rating\", ascending=False)[\"flavor\"]\r\n",
    "# Lastly, turn these grouped and ordered flavor labels into a list ready to use when creating graphs.\r\n",
    "stored_in_list_flavor_mean = list(ordered_flavor_mean)\r\n",
    "# In a more concise (and hard to read) line, the flow looks like this:\r\n",
    "desc_order_flavor_mean = list(beer_all.groupby(\"flavor\")[\"rating\"].mean().reset_index().sort_values(by=\"rating\", ascending=False)[\"flavor\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.3 Hypothesis 3: Flavor column might have observable variation between its subgroups."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot bars of Flavor averages.\r\n",
    "graph = sns.barplot(x=\"flavor\", y=\"rating\", data=beer_all, order=desc_order_flavor_mean)\r\n",
    "graph.axhline(y=beer_all[\"rating\"].mean(), linestyle=\"dashed\", color=\"#C48170\", label=\"feature mean\")\r\n",
    "graph.set(xlabel=\"Flavor\", ylabel=\"Rating\", title=\"Average Rating Of Style Subgroups\")\r\n",
    "graph.set_ylim([0, 10])\r\n",
    "graph.legend()\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# Result 3: Flavor subgroups have observable variation between their average.\r\n",
    "# The errorbar is bigger on herb subgroup."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.4 Hypothesis 4: Lemon beersâ€™ high average might not be due to outliers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot distribution of lemon flavored beer.\r\n",
    "graph = sns.boxplot(x=\"flavor\", y=\"rating\", data=beer_all, order=desc_order_flavor_mean)\r\n",
    "graph.axhline(y=beer_all[\"rating\"].median(), linestyle=\"dashed\", color=\"#C48170\", label=\"feature median\")\r\n",
    "graph.set(xlabel=\"Flavor\", ylabel=\"Rating\", title=\"Distribution Of Ratings In Flavor Subgroups\")\r\n",
    "graph.set_ylim([0, 12])\r\n",
    "graph.legend()\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# Result 4: The boxplot shows that the distribution of lemon beers is due to higher ratings overall compared to other subgroups.\r\n",
    "# There's no median on the lemon box. Let's find out why."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's investigate what's going on with the lineless box of lemon ratings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check if lemon median equals one of the quantiles, remember median is 0.50 quantile.\r\n",
    "lemon_ratings = beer_all[beer_all[\"flavor\"] == \"lemon\"][\"rating\"]\r\n",
    "print(lemon_ratings.quantile([0.25, 0.50, 0.75]))\r\n",
    "# That's it, 0.50 and 0.75 quantile are equal, so that's why the unusual boxplot.\r\n",
    "# Both 50% and 75% of all lemon ratings are higher or equal to 8.\r\n",
    "\r\n",
    "# Check distribution of lemon beer ratings to see if the quantile explanation matches the graph.\r\n",
    "graph = sns.kdeplot(x=lemon_ratings)\r\n",
    "graph.set(title=\"KDE of Lemon Beer Rating\", xlabel=\"Rating\")\r\n",
    "graph.set_xticks(range(-1, 12))\r\n",
    "plt.show()\r\n",
    "# There's a peak of observations where the rating is 8.\r\n",
    "# More than half of the distribution is on the left side of the 8 mark, so 75% looks plausible.\r\n",
    "\r\n",
    "# Conclusion: The ratings of lemon beers are so much higher than the rest, that their median overlaps with its 75th percentile."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Verify Data Quality"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4.1 Data Coverage"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.4.1.1 Data coverage in numerical variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Data coverage in ratings.\r\n",
    "unique_ratings = np.unique(beer_all[\"rating\"])\r\n",
    "unique_ratings_list = list(unique_ratings)\r\n",
    "print(f\"Uniques values of ratings are {unique_ratings_list}\")\r\n",
    "\r\n",
    "# Data coverage in abv.\r\n",
    "unique_abv = np.unique(beer_all[\"abv\"])\r\n",
    "unique_abv_list = list(unique_abv)\r\n",
    "print(f\"Uniques values of alcohol content are {unique_abv_list}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.4.1.2 Data coverage in categorical variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# How big are certain subgroups relative to the count of the others?\r\n",
    "\r\n",
    "def countplot_sns(df):\r\n",
    "    \"\"\"Count of observations in each subgroup of categorical columns, plotted as bars.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "    df (pandas.DataFrame): Dataframe containing categorical columns.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    Seaborn countplot in subplots for co unting observations in each subgroup ofcategorical columns.\r\n",
    "    Bars in descending order of counts.\r\n",
    "    Axes labels are added automatically based on column names.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # Select columns with data type category.\r\n",
    "    list_categoricals = df.select_dtypes(include=[\"category\"]).columns.tolist()\r\n",
    "    # Calculate the number of subplots in the figure.\r\n",
    "    number_of_plots = len(list_categoricals)\r\n",
    "    # Set the number of columns to 3 because it fits most screens.\r\n",
    "    number_of_cols = 3\r\n",
    "    # Calculate the number of rows in which subplots are shown.\r\n",
    "    number_of_rows = ceil(number_of_plots/number_of_cols)\r\n",
    "\r\n",
    "    # Plot figure and set title.\r\n",
    "    fig = plt.figure()\r\n",
    "    fig.suptitle(\"count of observations in each subgroup of categorical columns\".title())\r\n",
    "    # Iterate through list of categorical columns.\r\n",
    "    for i, elem in enumerate(list_categoricals):\r\n",
    "        # Define bar sorting criteria as descending counts.\r\n",
    "        desc_order = df[elem].value_counts().index\r\n",
    "        # Add subplots sequentially.\r\n",
    "        # Mark the first subplot as i+1 because subplot indices start at 1, and list indeces start at 0.\r\n",
    "        ax = fig.add_subplot(number_of_rows, number_of_cols, i+1)\r\n",
    "        \r\n",
    "        # Create each subplot, set labels and legend.\r\n",
    "        sns.countplot(x=elem, data=df, order=desc_order)\r\n",
    "        ax.set_title(elem.title(), fontsize=12, verticalalignment=\"bottom\", y=0.97)\r\n",
    "        ax.set_xticklabels(desc_order, fontsize=6, rotation=30, horizontalalignment=\"right\", verticalalignment=\"top\")\r\n",
    "        ax.set(xlabel=\"\", ylabel=\"\")\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "countplot_sns(beer_all)\r\n",
    "# Size of subgroups consistent with stores' assortment."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 3. DATA PREPARATION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Select Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.1 Distribution of ratings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "graph = sns.kdeplot(x=beer_all[\"rating\"])\r\n",
    "graph.set(title=\"KDE Of Rating\", xlabel=\"Rating\")\r\n",
    "graph.set_xticks(range(-1, 12))\r\n",
    "plt.show()\r\n",
    "# Curve of ratings KDE is gaussian."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.2 Distribution of alcohol content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Commercial beer is either regular, alcohol-free, or lemonade mix, and a smooth curve hides these groups.\r\n",
    "# Set bandwidth lower than 1 to check if groups show up.  \r\n",
    "graph = sns.kdeplot(x=beer_all[\"abv\"], label=\"smoothed curve\")\r\n",
    "graph = sns.kdeplot(x=beer_all[\"abv\"], bw_adjust=0.3, label=\"focused curve\")\r\n",
    "graph.set(title=\"KDE Of Alcohol Content\", xlabel=\"Alcohol content\")\r\n",
    "graph.set_xticks(range(0, 11))\r\n",
    "graph.legend()\r\n",
    "plt.show()\r\n",
    "# Curve of alcohol content KDE is not gaussian.\r\n",
    "# Still, it shows a pattern of three subgroups each with its own gaussian curve.\r\n",
    "\r\n",
    "# Check proportion of alcohol-free beer because it influences the curve of alcohol content.\r\n",
    "abv_list = beer_all[\"abv\"].tolist()\r\n",
    "abv_zero = (abv_list.count(0)/len(abv_list))*100\r\n",
    "print(f\"Alcohol-free are {abv_zero:.2f}% of total beer.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.3 Combined distribution of alcohol content and ratings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate Spearman's correlation coefficient because it works for non-linear relationship if the variables are monotonic.\r\n",
    "spearman_corr, _ = spearmanr(beer_all[\"abv\"], beer_all[\"rating\"])\r\n",
    "print(f\"Spearman\\'s correlation: {spearman_corr:.2f}.\")\r\n",
    "# Result: -0.11, so if they have any kind of relationship, it is not monotonic.\r\n",
    "# The scatterplot between abv and rating is consistent with Spearman's correlation coefficient.\r\n",
    "\r\n",
    "# Plot KDE of alcohol content and rating. Set bandwidth less than 1 because distribution of ABV is not gaussian.\r\n",
    "graph = sns.kdeplot(data=beer_all, x=beer_all[\"abv\"], y=beer_all[\"rating\"], bw_adjust=0.7, color=\"#C48170\")\r\n",
    "graph.set(title=\"KDE of alcohol content and rating\", xlabel=\"Alcohol content\", ylabel=\"Rating\")\r\n",
    "plt.show()\r\n",
    "# There are three zones, so it makes sense to split the dataset into three groups after all cleanup is done."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.4 Check country column if it has enough observations for each unique value, otherwise drop the column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Count occurrences for each country.\r\n",
    "country_count = beer_all[\"country\"].value_counts()\r\n",
    "\r\n",
    "# Count occurrences for each country as percentage.\r\n",
    "country_percentage = beer_all[\"country\"].value_counts(normalize=True).mul(100).round(1)\r\n",
    "\r\n",
    "# Collect all results in one dataframe.\r\n",
    "country_stats = pd.DataFrame({\"observations\": country_count, \"percentage\": country_percentage})\r\n",
    "print(country_stats)\r\n",
    "# There are 17 unique countries and 15 of them have each less than 5% of the total observations.\r\n",
    "\r\n",
    "# Drop country column.\r\n",
    "beer_all = beer_all.drop(\"country\", 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.5 Check which categorical features have high variation across subgroups for building models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot ratings variation in subgroups across categorical features.\r\n",
    "\r\n",
    "def barplot_sns(df, y_numeric_column, graph_title, categorical_columns=None):\r\n",
    "    \"\"\"Seaborn subplots of bars showing mean of numeric column grouped by categorical columns.\r\n",
    "    If list of categorical columns is not provided, uses columns of category type from dataframe.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "    df (pandas.DataFrame): Dataframe containing numerical and categorical columns.\r\n",
    "    y_series (str): Name of numerical column.\r\n",
    "    graph_title (str): Title of graph.\r\n",
    "    categorical_columns (list, optional): List of categorical columns.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    Seaborn subplots of bars with mean of each subgroup in categorical columns.\r\n",
    "    Horizontal line to mark the mean of each column. Bars in descending order of mean.\r\n",
    "    Axes labels are added automatically based on column names.\r\n",
    "    \"\"\"\r\n",
    "    # Check if input contains list of categorical columns. If not, select columns with data type category.\r\n",
    "    if categorical_columns != None:\r\n",
    "        list_categoricals = categorical_columns\r\n",
    "    else:\r\n",
    "        list_categoricals = df.select_dtypes(include=[\"category\"]).columns.tolist()\r\n",
    "    \r\n",
    "    # Calculate the number of subplots in the figure.\r\n",
    "    number_of_plots = len(list_categoricals)\r\n",
    "    # Set the number of columns to 3 because it fits most screens.\r\n",
    "    number_of_cols = 3\r\n",
    "    # Calculate the number of rows in which subplots are shown.\r\n",
    "    number_of_rows = ceil(number_of_plots/number_of_cols)\r\n",
    "\r\n",
    "    # Plot figure and set title.\r\n",
    "    fig = plt.figure()\r\n",
    "    fig.suptitle(graph_title.title())\r\n",
    "\r\n",
    "    # Iterate through list of categorical columns.\r\n",
    "    for i, elem in enumerate(list_categoricals):\r\n",
    "    # Define bar sorting criteria as descending mean.\r\n",
    "        desc_order = list(df.groupby(elem)[y_numeric_column].mean().reset_index().sort_values(by=y_numeric_column, ascending=False)[elem])\r\n",
    "        # Add subplots sequentially.\r\n",
    "        # Mark the first subplot as i+1 because subplot indices start at 1, and list indeces start at 0.\r\n",
    "        ax = fig.add_subplot(number_of_rows, number_of_cols, i+1)\r\n",
    "        # Create each subplot, set horizontal line to mark the mean, set labels and legend.\r\n",
    "        sns.barplot(x=elem, y=y_numeric_column, data=df, order=desc_order, dodge=False)\r\n",
    "        ax.axhline(y=df[y_numeric_column].mean(), linestyle=\"dashed\", color=\"#C48170\", label=\"feature mean\")\r\n",
    "        ax.set_title(elem.title(), fontsize=12, verticalalignment=\"bottom\", y=0.97)\r\n",
    "        ax.set_xticklabels(desc_order, fontsize=6, rotation=30, horizontalalignment=\"right\", verticalalignment=\"top\")\r\n",
    "        ax.set(xlabel=\"\", ylabel=\"\")\r\n",
    "        ax.set_ylim([0, 10])\r\n",
    "        ax.legend()\r\n",
    "        \r\n",
    "    plt.show()\r\n",
    " \r\n",
    "barplot_sns(beer_all, \"rating\", \"average rating of beer subgroups\", categoricals)\r\n",
    "\r\n",
    "\r\n",
    "# The barplot shows Flavor and Style have high variation between the average of their subgroups.\r\n",
    "# Method and Fermentation have low variation across their subgroups.\r\n",
    "# Errorbars are bigger on subgroups with low number of observations."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Abstract representation of ratings distribution. Take a look at median, IQR, whiskers, outliers.\r\n",
    "\r\n",
    "def boxplot_sns(df, y_numeric_column, graph_title, categorical_columns=None):\r\n",
    "    \"\"\"Seaborn subplots with boxplots of subgroups' distribution in categorical columns.\r\n",
    "    If list of categorical columns is not provided, uses columns of category type from dataframe.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "    df (pandas.DataFrame): Dataframe containing numerical and categorical columns.\r\n",
    "    y_series (str): Name of numerical column.\r\n",
    "    graph_title (str): Title for graph.\r\n",
    "    categorical_columns (list, optional): List of categorical columns.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    Seaborn subplots of boxplots showing distribution of subgroups in categorical columns.\r\n",
    "    Boxes in descending order of median.\r\n",
    "    Axes labels are added automatically based on column names.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # Check if input contains list of categorical columns. If not, select columns with data type category.\r\n",
    "    if categorical_columns != None:\r\n",
    "        list_categoricals = categorical_columns\r\n",
    "    else:\r\n",
    "        list_categoricals = df.select_dtypes(include=[\"category\"]).columns.tolist()\r\n",
    "    \r\n",
    "    # Calculate the number of subplots in the figure.\r\n",
    "    number_of_plots = len(list_categoricals)\r\n",
    "    # Set the number of columns to 3 because it fits most screens.\r\n",
    "    number_of_cols = 3\r\n",
    "    # Calculate the number of rows in which subplots are shown.\r\n",
    "    number_of_rows = ceil(number_of_plots/number_of_cols)\r\n",
    "    \r\n",
    "    # Plot figure and set title.\r\n",
    "    fig = plt.figure()\r\n",
    "    fig.suptitle(graph_title.title())\r\n",
    "    # Iterate through list of categorical columns.\r\n",
    "    for i, elem in enumerate(list_categoricals):\r\n",
    "        # Define box sorting criteria as descending median.\r\n",
    "        desc_order = list(df.groupby(elem)[y_numeric_column].median().reset_index().sort_values(by=y_numeric_column, ascending=False)[elem])\r\n",
    "        # Add subplots sequentially.\r\n",
    "        # Mark the first subplot as i+1 because subplot indices start at 1, and list indeces start at 0.\r\n",
    "        ax = fig.add_subplot(number_of_rows, number_of_cols, i+1)\r\n",
    "\r\n",
    "        # Create each subplot, set labels and legend.\r\n",
    "        sns.boxplot(x=elem, y=y_numeric_column, data=df, order=desc_order)\r\n",
    "        ax.axhline(y=df[y_numeric_column].median(), linestyle=\"dashed\", color=\"#C48170\", label=\"feature median\")\r\n",
    "        ax.set_title(elem.title(), fontsize=12, verticalalignment=\"bottom\", y=0.97)\r\n",
    "        ax.set_xticklabels(desc_order, fontsize=6, rotation=30, horizontalalignment=\"right\", verticalalignment=\"top\")\r\n",
    "        ax.set(xlabel=\"\", ylabel=\"\")\r\n",
    "        ax.set_ylim([0, 12])\r\n",
    "        ax.legend()\r\n",
    "\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "boxplot_sns(beer_all, \"rating\", \"Rating distribution of beer subgroups\", categoricals)\r\n",
    "\r\n",
    "# The boxplot shows Flavor and Style have high variation between the distributions of their subgroups.\r\n",
    "# Method and Fermentation have low variation across their subgroups.\r\n",
    "# Errorbars are bigger on subgroups with low number of observations."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot distribution of style subgroups to have an individual image of Style for the powerpoint presentation.\r\n",
    "desc_order_style = list(beer_all.groupby(\"style\")[\"rating\"].median().reset_index().sort_values(by=\"rating\", ascending=False)[\"style\"])\r\n",
    "graph = sns.boxplot(x=\"style\", y=\"rating\", data=beer_all, order=desc_order_style)\r\n",
    "graph.axhline(y=beer_all[\"rating\"].median(), linestyle=\"dashed\", color=\"#C48170\", label=\"feature median\")\r\n",
    "graph.set(xlabel=\"Style\", ylabel=\"Rating\", title=\"Distribution Of Ratings In Style Subgroups\")\r\n",
    "graph.set_ylim([0, 12])\r\n",
    "graph.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Clean data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define range of normal ratings as 2*std away from the mean because dataset is small and has gaussian distribution.\r\n",
    "mean_rating = beer_all[\"rating\"].mean()\r\n",
    "std_rating = beer_all[\"rating\"].std()\r\n",
    "two_std_rating = std_rating * 2\r\n",
    "\r\n",
    "lower_limit_rating = mean_rating - two_std_rating\r\n",
    "upper_limit_rating = mean_rating + two_std_rating\r\n",
    "\r\n",
    "print(f\"The mean rating is {mean_rating:.2f}.\")\r\n",
    "print(f\"The lower limit of normal ratings is {lower_limit_rating:.2f} and the upper limit is {upper_limit_rating:.2f}.\")\r\n",
    "\r\n",
    "# Identify rating outliers. Use result to train machine learning model and avoid overfitting.\r\n",
    "outlier_ratings = [x for x in beer_all[\"rating\"] if x < lower_limit_rating or x > upper_limit_rating]\r\n",
    "outlier_ratings.sort()\r\n",
    "# print(f\"These are the rating outliers: {outlier_ratings}\")\r\n",
    "print(f\"There are {len(outlier_ratings)} outliers out of {len(beer_all.rating)} total rating observations.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Construct Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.1 Derived Attributes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create column for filtration status.\r\n",
    "unfiltered_words = [\"unfiltered\", \"kellerbier\", \"natur\", \"naturtrubes\", \"nefiltrata\", \"nonfiltrata\"]\r\n",
    "beer_all[\"filtration\"] = beer_all[\"name\"].str.contains(\"|\".join(unfiltered_words))\r\n",
    "beer_all[\"filtration\"] = beer_all[\"filtration\"].replace({True: \"unfiltered\", False: \"filtered\"})\r\n",
    "\r\n",
    "# Create column for pasteurization status.\r\n",
    "unpasteurized_words = [\"unpasteurized\", \"kellerbier\", \"natur\", \"naturtrubes\", \"nepasteurizata\", \"nonpastorizzata\"]\r\n",
    "beer_all[\"pasteurization\"] = beer_all[\"name\"].str.contains(\"|\".join(unpasteurized_words))\r\n",
    "beer_all[\"pasteurization\"] = beer_all[\"pasteurization\"].replace({True: \"unpasteurized\", False: \"pasteurized\"})\r\n",
    "\r\n",
    "# Format the new columns as categoricals.\r\n",
    "beer_all[[\"filtration\", \"pasteurization\"]] = beer_all[[\"filtration\", \"pasteurization\"]].astype(\"category\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2 Generated records"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create column to bin alcohol content as categorical data.\r\n",
    "abv_bins = [0, 0.5, 2.8, 4.4, 5.5, 10]\r\n",
    "perception_labels = [\"drive\", \"refresh\", \"weak\", \"tasty\", \"too_strong\"]\r\n",
    "beer_all[\"perception\"] = pd.cut(beer_all[\"abv\"], bins=abv_bins, labels=perception_labels, include_lowest=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 Integrate Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.1 Select observations with ratings less than 2*std away from the mean"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Use limits calculated at 3.2 and reset index.\r\n",
    "outlier_condition = (beer_all.rating < lower_limit_rating) | (beer_all.rating > upper_limit_rating)\r\n",
    "beer_2std = beer_all.drop(beer_all[outlier_condition].index)\r\n",
    "beer_2std.reset_index(inplace=True, drop=True)\r\n",
    "print(f\"Dataframe without outliers has {len(beer_2std.rating)} rows.\")\r\n",
    "\r\n",
    "# Check this out, below selection by square brackets doesn't work in f-string, must use dot notation.\r\n",
    "# print(f\"There are {len(beer_2std[\"rating\"])} observations with normal ratings.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.2 Split dataset by alcohol content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Split dataset by alcohol content into three subsets: alcohol-free, light and regular beer.\r\n",
    "alc_free_all = beer_all[beer_all[\"abv\"] <= 0.5]\r\n",
    "light_all = beer_all[(beer_all[\"abv\"] > 0.5) & (beer_all[\"abv\"] <= 3)]\r\n",
    "regular_all = beer_all[beer_all[\"abv\"] > 3]\r\n",
    "\r\n",
    "alc_free_2std = beer_2std[beer_2std[\"abv\"] <= 0.5]\r\n",
    "light_2std = beer_2std[(beer_2std[\"abv\"] > 0.5) & (beer_2std[\"abv\"] <= 3)]\r\n",
    "regular_2std = beer_2std[beer_2std[\"abv\"] > 3]\r\n",
    "\r\n",
    "print(f\"In complete ratings range there are {len(alc_free_all)} alcohol-free, {len(light_all)} light and {len(regular_all)} regular beers.\")\r\n",
    "print(f\"In less than 2*std away ratings there are {len(alc_free_2std)} alcohol-free, {len(light_2std)} light and {len(regular_2std)} regular beers.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.3 Check distribution of ratings in subsets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot distribution of ratings in each subset to check if their curves are gaussian.\r\n",
    "graph = sns.kdeplot(x=alc_free_all[\"rating\"], label=\"all\")\r\n",
    "graph = sns.kdeplot(x=alc_free_2std[\"rating\"], label=\"2_std\")\r\n",
    "graph.set(title=\"Alcohol-free Beer Ratings\", xlabel=\"Rating\")\r\n",
    "graph.set_xticks(range(-1, 12))\r\n",
    "plt.legend()\r\n",
    "plt.show()\r\n",
    "\r\n",
    "graph = sns.kdeplot(x=light_all[\"rating\"], label=\"all\")\r\n",
    "graph = sns.kdeplot(x=light_2std[\"rating\"], label=\"2_std\")\r\n",
    "graph.set(title=\"Light Beer Ratings\", xlabel=\"Rating\")\r\n",
    "graph.set_xticks(range(-1, 12))\r\n",
    "plt.legend()\r\n",
    "plt.show()\r\n",
    "\r\n",
    "graph = sns.kdeplot(x=regular_all[\"rating\"], label=\"all\")\r\n",
    "graph = sns.kdeplot(x=regular_2std[\"rating\"], label=\"2_std\")\r\n",
    "graph.set(title=\"Regular Beer Ratings\", xlabel=\"Rating\")\r\n",
    "graph.set_xticks(range(-1, 12))\r\n",
    "plt.legend()\r\n",
    "plt.show()\r\n",
    "# KDE plots of the six subsets prove that ratings keep their gaussian curve even if split by alcohol content criteria."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a dictionary that stores datasets as values and their names as keys.\r\n",
    "clean_dataframes = [beer_all, alc_free_all, light_all, regular_all, beer_2std, alc_free_2std, light_2std, regular_2std]\r\n",
    "dataframe_names = [\"beer_all\", \"alc_free_all\", \"light_all\", \"regular_all\", \"beer_2std\", \"alc_free_2std\", \"light_2std\", \"regular_2std\"]\r\n",
    "\r\n",
    "dataframes_dict = dict(zip(dataframe_names, clean_dataframes))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.4 Feature variation across subgroups of alcohol content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check feature variation across subgroups when split into the three subsets based on alcohol content.\r\n",
    "\r\n",
    "for key, value in dataframes_dict.items():\r\n",
    "    boxplot_sns(value, \"rating\", f\"Distribution of {key}\")\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "# All subsets have higher variation in Style and Flavor, and no variation in Method.\r\n",
    "# Some subsets have a bit of variation in Filtration, Pasteurization, Fermentation and Perception."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5 Format Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Formatting was done in previous preprocessing tasks, and the only task left is sorting.\r\n",
    "# Sort dataframe on ABV, then on rating.\r\n",
    "beer_all.sort_values([\"rating\", \"abv\"], ascending=[True, True], inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.6 Dataset - Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Export dataframes and remove index because beer identification is done through their unique names.\r\n",
    "# It's also good to avoid having two columns with indices next time the file is imported.\r\n",
    "\r\n",
    "\r\n",
    "# Create function to export dictionary of dataframes to csv files.\r\n",
    "\r\n",
    "def export_dict_as_csv(dataframes_dictionary, output_folder, suffix=None):\r\n",
    "    \"\"\"Export dictionary as csv files. Dictionary contains names as keys and dataframes as values.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "    dataframes_dictionary (dict): dictionary of dataframes as values and their names as keys.\r\n",
    "    output_folder (str): path where to export files.\r\n",
    "    suffix (str, optional): suffix to add after filename.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    csv files\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    for key, value in dataframes_dictionary.items():\r\n",
    "        # Extract filename.\r\n",
    "        filename = str(key)\r\n",
    "\r\n",
    "        # Create file path.\r\n",
    "        if suffix != None:\r\n",
    "            output_address = output_folder + os.sep + filename + suffix + \".csv\"\r\n",
    "        else:\r\n",
    "            output_address = output_folder + os.sep + filename + \".csv\"\r\n",
    "\r\n",
    "        # Export dataframe.\r\n",
    "        value.to_csv(output_address, index=False)\r\n",
    "\r\n",
    "\r\n",
    "# Path to folder containing clean files.\r\n",
    "clean_files_path = os.getcwd() + os.sep + \"beer_output\" + os.sep + \"clean_files\"\r\n",
    "\r\n",
    "# Export clean files.\r\n",
    "export_dict_as_csv(dataframes_dict, clean_files_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.7 Dataset Description"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print total number of columns, rows and ratings lower than 5.\r\n",
    "# Print description of categorical columns.\r\n",
    "\r\n",
    "for key, value in dataframes_dict.items():\r\n",
    "    total_rows = value.shape[0]\r\n",
    "    lower_than_five = value[value[\"rating\"] < 5]\r\n",
    "    only_lows = lower_than_five.shape[0]\r\n",
    "    total_columns = value.shape[1]\r\n",
    "    print(f\"{key} \\nRows: {total_rows} \\nNumber of ratings lower than 5: {only_lows} \\nColumns: {total_columns} \\n\")\r\n",
    "    describe_categorical_columns(value, \"rating\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 4. MODELING. MACHINE LEARNING WITH H2O"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Initialize H2O"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Assertions are disabled because they are mainly used for error checking and debugging purposes.\r\n",
    "# nthreads=-1 means use all CPU on the host.\r\n",
    "h2o.init(nthreads=-1, enable_assertions=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Select .csv Files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Select clean files.\r\n",
    "clean_files = glob.glob(clean_files_path + os.sep + \"*.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 Import Datasets As Dictionaries Into H2O"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write function to import multiple files at once.\r\n",
    "\r\n",
    "def files_to_h2o_frames(files, response_column):\r\n",
    "    \"\"\"Import files into H2O frames.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "    files (list): List of files to be imported.\r\n",
    "    response_column (str): Name of response column.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    dict: Dictionary containing H2O frames, predictors and response. Key: the name of a dataset. Values: the imported frame, its list of predictors, the response column.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # Create empty dictionary to be populated at each iteration.\r\n",
    "    dictionary = {}\r\n",
    "\r\n",
    "    # Iterate through list of files to create name, frame and list of predictors.\r\n",
    "    for i, elem in enumerate(files):\r\n",
    "        name = PureWindowsPath(elem).stem\r\n",
    "        frame = h2o.import_file(elem)\r\n",
    "        \r\n",
    "        if \"alc_free\" in name:\r\n",
    "            predictors = [\"style\", \"flavor\"]\r\n",
    "        elif \"light\" in name:\r\n",
    "            predictors = [\"style\", \"flavor\", \"abv\"]\r\n",
    "        elif \"regular\" in name:\r\n",
    "            predictors = [\"style\", \"flavor\", \"pasteurization\", \"abv\"]\r\n",
    "        else:\r\n",
    "            predictors = [\"style\", \"flavor\", \"perception\", \"abv\"]\r\n",
    "\r\n",
    "        # Add key and values to dictionary.\r\n",
    "        dictionary[name] = {\"frame\":frame, \"predictors\":predictors, \"response\": response_column}\r\n",
    "    return dictionary\r\n",
    "\r\n",
    "frames_dictionary = files_to_h2o_frames(clean_files, \"rating\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4 Select Modeling Techniques"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# I have selected Distributed Random Forest (DRF), Gradient Boosting Machine (GBM) - see motives in Final Report file."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.5 Generate Test Design"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The model will learn from the training set and will be assessed on the test set.\r\n",
    "# Train 0.7, valid 0.15 and test 0.15 splits were decided manually to ensure they are diverse no matter how few observations there are.\r\n",
    "# See note in Readme file."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.6 Build Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.6.1 Parameter Settings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Parameters are listed in the Final Report, together with an explanation fow why I have chosen them.\r\n",
    "# Parameters are grouped by model type DRF and GBM, and for each model they are grouped by instantiate, training, testing, and export tasks. "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.6.2 Define Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.6.2.1 Distributed Random Forest - DRF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write function to generate DRF model and export prediction as pandas dataframe.\r\n",
    "\r\n",
    "def model_h2o_drf(frames, model_output_folder, pred_output_folder, mse_output_folder):\r\n",
    "    \"\"\"Build DRF model in H2O for each dataset, add prediction to pandas dataframe, export result and MSE as csv.\r\n",
    "\r\n",
    "    Args:\r\n",
    "    frames (dict): Dictionary containing frames, predictors and response column.\r\n",
    "    model_output_folder (str): Path to folder where to export DRF model.\r\n",
    "    pred_output_folder (str): Path to folder where to export predictions from DRF model.\r\n",
    "    mse_output_folder (str): Path to folder where to export file with MSE of all DRF models.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    zip archive of each model\r\n",
    "    csv file with predictions of each model\r\n",
    "    csv file with MSE of all DRF models\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # Create list of model names.\r\n",
    "    model_names = []\r\n",
    "\r\n",
    "    # Create list of MSE results from each model.\r\n",
    "    mse_list = []\r\n",
    "\r\n",
    "    # Iterate through dictionary and store its elements under short variable names to help with readability.\r\n",
    "    for key, value in frames.items():\r\n",
    "        # Access the name.\r\n",
    "        name = str(key)+ \"_drf\"\r\n",
    "\r\n",
    "        # Append model name to list.\r\n",
    "        model_names.append(name)\r\n",
    "\r\n",
    "        # Access the frame.\r\n",
    "        frame = frames[key][\"frame\"]\r\n",
    "        # Access the predictors.\r\n",
    "        predictor_list = frames[key][\"predictors\"]\r\n",
    "        # Access the response.\r\n",
    "        response_column = frames[key][\"response\"]\r\n",
    "\r\n",
    "        # Split rows into training, validation and test sets. This makes reproducibility possible.\r\n",
    "        train = frame[frame[\"split\"]==\"train\"]\r\n",
    "        valid = frame[frame[\"split\"]==\"valid\"]\r\n",
    "        test = frame[frame[\"split\"]==\"test\"]\r\n",
    "\r\n",
    "        # Instantiate model with custom parameters explained in Final Report, step 4.6.1.\r\n",
    "        model = H2ORandomForestEstimator(seed=12, categorical_encoding=\"Enum\", nfolds=4, fold_assignment=\"random\", \r\n",
    "        mtries=len(predictor_list), nbins=13, nbins_top_level=16, build_tree_one_node=True)\r\n",
    "\r\n",
    "        # Train model. Specify predictors, response column, training frame and validation frame.\r\n",
    "        model.train(x=predictor_list, y=response_column, training_frame=train, validation_frame=valid, model_id=name+\"_model\")\r\n",
    "\r\n",
    "        # Select variable importance from model json.\r\n",
    "        model_output = model._model_json[\"output\"]\r\n",
    "        var_imp_values = model_output[\"variable_importances\"].cell_values\r\n",
    "        var_imp_header = model_output[\"variable_importances\"].col_header\r\n",
    "        variable_importance = pd.DataFrame(var_imp_values, columns=var_imp_header)\r\n",
    "\r\n",
    "        # Generate prediction. It gets stored in a H2O frame with one column named \"predict\".\r\n",
    "        prediction = model.predict(frame)\r\n",
    "\r\n",
    "        # Calculate model performance on test set.\r\n",
    "        performance = model.model_performance(test)\r\n",
    "\r\n",
    "        # Store model performance as json into a dictionary.\r\n",
    "        perf_dict = performance._metric_json\r\n",
    "\r\n",
    "        # Select only MSE from performance dictionary. Use ndarray.item method to catch errors in case MSE output is not a float.\r\n",
    "        mse_value = np.asarray([value for key, value in perf_dict.items() if key == \"MSE\"]).item()\r\n",
    "        \r\n",
    "        # Append MSE list.\r\n",
    "        mse_list.append(mse_value)\r\n",
    "\r\n",
    "        # Add prediction to original H2O frame to help further analysis.\r\n",
    "        dataset_plus_prediction = frame.cbind(prediction)\r\n",
    "\r\n",
    "        # Convert H2O predictions frame to pandas dataframe.\r\n",
    "        dataset_plus_prediction_pandas = dataset_plus_prediction.as_data_frame()\r\n",
    "\r\n",
    "        # Export prediction dataframe.\r\n",
    "        dataset_plus_prediction_pandas.to_csv(pred_output_folder + os.sep + name + \".csv\", index=False)\r\n",
    "\r\n",
    "        # Export model.\r\n",
    "        model.download_mojo(path=model_output_folder + os.sep + name + \".zip\", get_genmodel_jar=False)\r\n",
    "\r\n",
    "        # Export variable importance.\r\n",
    "        variable_importance.to_csv(model_output_folder + os.sep + name + \"_varimp.csv\", index=False)\r\n",
    "        \r\n",
    "\r\n",
    "    # Zip names and MSE values into a pandas dataframe.\r\n",
    "    mse_models = pd.DataFrame(zip(model_names, mse_list), columns=[\"model_name\", \"mse\"])\r\n",
    "\r\n",
    "    # Export MSE dataframe to csv file.\r\n",
    "    mse_models.to_csv(mse_output_folder + os.sep + \"drf_mse.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.6.2.2 Gradient Boosting Machine - GBM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write function to generate GBM model and export prediction as pandas dataframe.\r\n",
    "\r\n",
    "def model_h2o_gbm(frames, model_output_folder, pred_output_folder, mse_output_folder):\r\n",
    "    \"\"\"Build GBM model in H2O for each dataset, add prediction to pandas dataframe, export result and MSE as csv.\r\n",
    "\r\n",
    "    Args:\r\n",
    "    frames (dict): Dictionary containing frames, predictors and response column.\r\n",
    "    model_output_folder (str): Path to folder where to export GBM model.\r\n",
    "    pred_output_folder (str): Path to folder where to export predictions from GBM model.\r\n",
    "    mse_output_folder (str): Path to folder where to export file with MSE of all GBM models.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    zip archive of each model\r\n",
    "    csv file with predictions of each model\r\n",
    "    csv file with MSE of all GBM models\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    # Create list of model names.\r\n",
    "    model_names = []\r\n",
    "\r\n",
    "    # Create list of MSE results from each model.\r\n",
    "    mse_list = []\r\n",
    "\r\n",
    "    # Iterate through dictionary and store its elements under short variable names to help with readability.\r\n",
    "    for key, value in frames.items():\r\n",
    "        # Access the name.\r\n",
    "        name = str(key) + \"_gbm\"\r\n",
    "\r\n",
    "        # Append model name to list.\r\n",
    "        model_names.append(name)\r\n",
    "\r\n",
    "        # Access the frame.\r\n",
    "        frame = frames[key][\"frame\"]\r\n",
    "        # Access the predictors.\r\n",
    "        predictor_list = frames[key][\"predictors\"]\r\n",
    "        # Access the response.\r\n",
    "        response_column = frames[key][\"response\"]\r\n",
    "\r\n",
    "        # Split rows into training, validation and test sets. This makes reproducibility possible.\r\n",
    "        train = frame[frame[\"split\"]==\"train\"]\r\n",
    "        valid = frame[frame[\"split\"]==\"valid\"]\r\n",
    "        test = frame[frame[\"split\"]==\"test\"]\r\n",
    "\r\n",
    "        # Instantiate model with custom parameters explained in Final Report, step 4.6.1.\r\n",
    "        model = H2OGradientBoostingEstimator(seed=12, categorical_encoding=\"Enum\", nfolds=4, fold_assignment=\"random\", \r\n",
    "        min_rows=1, nbins=13, nbins_top_level=16, distribution=\"gaussian\", build_tree_one_node=True)\r\n",
    "\r\n",
    "        # Train model. Specify predictors, response column, training frame and validation frame.\r\n",
    "        model.train(x=predictor_list, y=response_column, training_frame=train, validation_frame=valid, model_id=name+\"_model\")\r\n",
    "\r\n",
    "        # Select variable importance from model json.\r\n",
    "        model_output = model._model_json[\"output\"]\r\n",
    "        var_imp_values = model_output[\"variable_importances\"].cell_values\r\n",
    "        var_imp_header = model_output[\"variable_importances\"].col_header\r\n",
    "        variable_importance = pd.DataFrame(var_imp_values, columns=var_imp_header)\r\n",
    "\r\n",
    "        # Generate prediction. It gets stored in a H2O frame with one column named \"predict\".\r\n",
    "        prediction = model.predict(frame)\r\n",
    "\r\n",
    "        # Calculate model performance on test.\r\n",
    "        performance = model.model_performance(test)\r\n",
    "\r\n",
    "        # Store model performance as json into a dictionary.\r\n",
    "        perf_dict = performance._metric_json\r\n",
    "\r\n",
    "        # Select only MSE from performance dictionary. Use ndarray.item method to catch errors in case MSE output is not a float.\r\n",
    "        mse_value = np.asarray([value for key, value in perf_dict.items() if key == \"MSE\"]).item()\r\n",
    "        \r\n",
    "        # Append MSE list.\r\n",
    "        mse_list.append(mse_value)\r\n",
    "\r\n",
    "        # Add prediction to original H2O frame to help further analysis.\r\n",
    "        dataset_plus_prediction = frame.cbind(prediction)\r\n",
    "\r\n",
    "        # Convert H2O predictions frame to pandas dataframe.\r\n",
    "        dataset_plus_prediction_pandas = dataset_plus_prediction.as_data_frame()\r\n",
    "\r\n",
    "        # Export prediction dataframe.\r\n",
    "        dataset_plus_prediction_pandas.to_csv(pred_output_folder + os.sep + name + \".csv\", index=False)\r\n",
    "        \r\n",
    "        # Export model.\r\n",
    "        model.download_mojo(path=model_output_folder + os.sep + name + \".zip\", get_genmodel_jar=False)\r\n",
    "\r\n",
    "        # Export variable importance.\r\n",
    "        variable_importance.to_csv(model_output_folder + os.sep + name + \"_varimp.csv\", index=False)\r\n",
    "        \r\n",
    "    # Zip names and MSE values into a pandas dataframe.\r\n",
    "    mse_models = pd.DataFrame(zip(model_names, mse_list), columns=[\"model_name\", \"mse\"])\r\n",
    "\r\n",
    "    # Export MSE dataframe to csv file.\r\n",
    "    mse_models.to_csv(mse_output_folder + os.sep + \"gbm_mse.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.6.3 Call Functions to Build Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Output folders for models.\r\n",
    "model_drf_folder = os.getcwd() + os.sep + \"beer_output\" + os.sep + \"models\" + os.sep + \"drf_models\"\r\n",
    "model_gbm_folder = os.getcwd() + os.sep + \"beer_output\" + os.sep + \"models\" + os.sep + \"gbm_models\"\r\n",
    "\r\n",
    "# Output folders for predictions dataframes resulted from DRF and GBM models.\r\n",
    "pred_drf_folder = os.getcwd() + os.sep + \"beer_output\" + os.sep + \"predictions\" + os.sep + \"drf_predictions\"\r\n",
    "pred_gbm_folder = os.getcwd() + os.sep + \"beer_output\" + os.sep + \"predictions\" + os.sep + \"gbm_predictions\"\r\n",
    "\r\n",
    "# Output folder for MSE of models.\r\n",
    "mse_folder = os.getcwd() + os.sep + \"beer_output\" + os.sep + \"metrics\" + os.sep + \"mse\"\r\n",
    "\r\n",
    "# Call functions that build models and export dataframes. \r\n",
    "model_h2o_drf(frames_dictionary, model_drf_folder, pred_drf_folder, mse_folder)\r\n",
    "model_h2o_gbm(frames_dictionary, model_gbm_folder, pred_gbm_folder, mse_folder)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.7 Assess Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create function to select files with a certain pattern.\r\n",
    "\r\n",
    "def import_csv_files_as_dict(files_path, files_pattern, check_subfolders):\r\n",
    "    \"\"\"Import csv files that match a pattern, return dictionary with filenames as keys and dataframes as values.\r\n",
    "    Args:\r\n",
    "    files_path (str):\r\n",
    "    files_pattern (str):\r\n",
    "    check_subfolders (bool): True if imports from subfolders. False if imports only from main folder.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    names_dataframes_dict (dict): dictionary of dataframes as values and their names as keys.\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    # Select files that match pattern. Recursive parameter decides if selection contains files from subfolders.\r\n",
    "    files = glob.glob(files_path + os.sep + files_pattern, recursive=check_subfolders)\r\n",
    "\r\n",
    "    # Extract name of files and use them to store dataframes.\r\n",
    "    names = [PureWindowsPath(elem).stem for elem in files]\r\n",
    "\r\n",
    "    # Read files into pandas dataframes.\r\n",
    "    dataframes = [pd.read_csv(item) for item in files]\r\n",
    "\r\n",
    "    # Create dictionary to store names and dataframes of predictions.\r\n",
    "    names_dataframes_dict = dict(zip(names, dataframes))\r\n",
    "\r\n",
    "    # Return dictionary of names and dataframes.\r\n",
    "    return names_dataframes_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# See discussion in Final Report about built-in metrics of models.\r\n",
    "# Import variable importance files.\r\n",
    "var_imp_path = os.getcwd() + os.sep + \"beer_output\" + os.sep + \"models\"\r\n",
    "var_imp_pattern = \"/**/*.csv\"\r\n",
    "var_imp_dict = import_csv_files_as_dict(var_imp_path, var_imp_pattern, True)\r\n",
    "\r\n",
    "# Print variable importance dataframes.\r\n",
    "for key, value in var_imp_dict.items():\r\n",
    "    print(key, value, \"\\n\")\r\n",
    "\r\n",
    "# Most important predictors are Flavor in alc_free_drf_all, ABV in light_all_drf, Style in regular_all_drf and beer_all_drf."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 5. EVALUATION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 Evaluate results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1.1 Assessment of data mining results w.r.t. business success criteria"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Steps for evaluating model real-life performance:\r\n",
    "# 1. Import predictions.\r\n",
    "# 2. Select false negatives, which means ratings lower than 5 that were predicted as 5 or higher.\r\n",
    "# 3. Calculate recall score.\r\n",
    "# 4. Approve models that have the highest recall score."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.1.1.1 Import predictions dataframes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import predictions dataframes generated by models.\r\n",
    "predictions_path = os.getcwd() + os.sep + \"beer_output\" + os.sep + \"predictions\"\r\n",
    "predictions_pattern = \"/**/*.csv\"\r\n",
    "\r\n",
    "predictions_dict = import_csv_files_as_dict(predictions_path, predictions_pattern, True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.1.1.2 Select false negatives"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create function to generate dictionary of false negatives dataframes.\r\n",
    "\r\n",
    "def select_false_negatives(names_dataframes_dict, real_response_column, predict_column, threshold):\r\n",
    "    \"\"\"Create false negatives dictionary, names as keys and dataframes as values.\r\n",
    "    The name of the response column should be the same in all dataframes. Same applies for predicted column.\r\n",
    "\r\n",
    "    Args:\r\n",
    "    names_dataframes_dict (dict): Dictionary of dataframes and their names.\r\n",
    "    real_response_column (str): The name of the response column with numerical data as values.\r\n",
    "    predict_column(str): The name of the predicted column with numerical data as values.\r\n",
    "    threshold (int, float): The threshold that separates outcomes.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    false_negatives_dict (dict): dictionary of false negatives dataframes as values and their names as keys.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # Create empty dictionary to store false negatives.\r\n",
    "    false_negatives_dict = {}\r\n",
    "\r\n",
    "    # Iterate through input dictionary.\r\n",
    "    for key, value in names_dataframes_dict.items():\r\n",
    "        \r\n",
    "        # Create name of false negatives dataframe.\r\n",
    "        name = str(key) + \"_fn\"\r\n",
    "\r\n",
    "        # Access the dataframe stored as value in input dictionary.\r\n",
    "        dataframe = names_dataframes_dict[key]\r\n",
    "\r\n",
    "        # Select true positives and false negatives:\r\n",
    "        false_negatives = dataframe[(dataframe[real_response_column] < threshold) & (dataframe[predict_column] >= threshold)]\r\n",
    "\r\n",
    "        # If false negatives dataframe is not empty, append it to the false negatives dictionary.\r\n",
    "        if not false_negatives.empty:\r\n",
    "            false_negatives_dict[name] = false_negatives\r\n",
    "        else:\r\n",
    "            pass\r\n",
    "    \r\n",
    "    # Return dictionary of false negatives.\r\n",
    "    return false_negatives_dict\r\n",
    "\r\n",
    "\r\n",
    "false_negatives = select_false_negatives(predictions_dict, \"rating\", \"predict\", 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Export false negatives.\r\n",
    "false_negatives_path = os.getcwd() + os.sep + \"beer_output\" + os.sep + \"false_negatives\"\r\n",
    "export_dict_as_csv(false_negatives, false_negatives_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.1.1.3 Calculate recall score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create function to calculate recall score.\r\n",
    "\r\n",
    "def calculate_recall_score(names_dataframes_dict, real_response_column, predict_column, threshold):\r\n",
    "    \"\"\"Calculate recall score of models by counting true positives and false negatives in their predictions.\r\n",
    "\r\n",
    "    Args:\r\n",
    "    names_dataframes_dict (dict): Dictionary of dataframes and their names.\r\n",
    "    real_response_column (str): The name of the response column with numerical data as values.\r\n",
    "    predict_column(str): The name of the predicted column with numerical data as values.\r\n",
    "    threshold (int, float): The threshold that separates outcomes.\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "    recall_dataframe (pandas.DataFrame): dataframe of model names and their recall scores.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # Create empty dictionary to store recall scores.\r\n",
    "    recall_dict = {}\r\n",
    "    \r\n",
    "    # Iterate through input dictionary.\r\n",
    "    for key, value in names_dataframes_dict.items():\r\n",
    "\r\n",
    "        # Access the name of the model.\r\n",
    "        name = str(key)\r\n",
    "        \r\n",
    "        # Access the dataframe stored as value in input dictionary.\r\n",
    "        dataframe = names_dataframes_dict[key]\r\n",
    "\r\n",
    "        # Select true positives and false negatives:\r\n",
    "        true_positives = dataframe[(dataframe[real_response_column] < threshold) & (dataframe[predict_column] < threshold)]\r\n",
    "        false_negatives = dataframe[(dataframe[real_response_column] < threshold) & (dataframe[predict_column] >= threshold)]\r\n",
    "\r\n",
    "        # Calculate recall score, where df.shape[0] counts the rows of a df.\r\n",
    "        try:\r\n",
    "            recall = true_positives.shape[0] / (false_negatives.shape[0] + true_positives.shape[0])\r\n",
    "        except ZeroDivisionError:\r\n",
    "            recall = np.NaN    \r\n",
    "\r\n",
    "        # Append score to recall dictionary.\r\n",
    "        recall_dict[name] = recall\r\n",
    "\r\n",
    "    # Convert dictionaty to dataframe.\r\n",
    "    recall_dataframe = pd.DataFrame(list(recall_dict.items()), columns=[\"model_name\", \"recall_score\"])\r\n",
    "\r\n",
    "    # Return recall score dataframe.\r\n",
    "    return recall_dataframe\r\n",
    "\r\n",
    "\r\n",
    "recall = calculate_recall_score(predictions_dict, \"rating\", \"predict\", 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Export recall dataframe.\r\n",
    "recall_path = os.getcwd() + os.sep + \"beer_output\" + os.sep + \"metrics\" + os.sep + \"recall\" + os.sep + \"recall_score.csv\"\r\n",
    "recall.to_csv(recall_path, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.1.1.4 Merge MSE and recall in a single dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import MSE files.\r\n",
    "drf_mse = pd.read_csv(os.getcwd() + os.sep + \"beer_output\" + os.sep + \"metrics\" + os.sep + \"mse\" + os.sep + \"drf_mse.csv\")\r\n",
    "gbm_mse = pd.read_csv(os.getcwd() + os.sep + \"beer_output\" + os.sep + \"metrics\" + os.sep + \"mse\" + os.sep + \"gbm_mse.csv\")\r\n",
    "\r\n",
    "# Merge MSE files.\r\n",
    "merged_mse = drf_mse.merge(gbm_mse, how=\"outer\")\r\n",
    "\r\n",
    "# Merge total MSE with recall score.\r\n",
    "mse_recall = merged_mse.merge(recall, how=\"outer\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create column to store the model type.\r\n",
    "mse_recall[\"model_type\"] = mse_recall[\"model_name\"].str.contains(\"drf\")\r\n",
    "mse_recall[\"model_type\"] = mse_recall[\"model_type\"].replace({True: \"DRF\", False: \"GBM\"})\r\n",
    "\r\n",
    "# Create a column to store the range of each dataset.\r\n",
    "mse_recall[\"dataset_range\"] = mse_recall[\"model_name\"].str.contains(\"_2std\")\r\n",
    "mse_recall[\"dataset_range\"] = mse_recall[\"dataset_range\"].replace({True: \"2-std\", False: \"all\"})\r\n",
    "\r\n",
    "# Sort dataframe by recall score.\r\n",
    "mse_recall.sort_values([\"recall_score\", \"model_name\"], ascending=False, inplace=True)\r\n",
    "\r\n",
    "# Export dataframe.\r\n",
    "mse_recall.to_csv(os.getcwd() + os.sep + \"beer_output\" + os.sep + \"metrics\" + os.sep + \"mse_recall.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.1.1.5 Plot graph of recall score and MSE of each model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot MSE and the recall score of each model to compare metrics.\r\n",
    "fig, ax = plt.subplots()\r\n",
    "sns.lineplot(x=\"model_name\", y=\"mse\", data=mse_recall, label=\"MSE\", linestyle=\"dotted\", marker=\"o\")\r\n",
    "sns.lineplot(x=\"model_name\", y=\"recall_score\", data=mse_recall, label=\"Recall\", linestyle=\"dashed\", marker=\"D\")\r\n",
    "ax.set(title=\"MSE And Recall Score Of Models\", xlabel=\"\", ylabel=\"MSE and recall\")\r\n",
    "ax.set_xticks(np.arange(len(mse_recall[\"model_name\"])))\r\n",
    "plt.xticks(rotation=30, horizontalalignment=\"right\", verticalalignment=\"top\")\r\n",
    "ax.legend()\r\n",
    "plt.show()\r\n",
    "# MSE is lowest on models with maximum recall score, which is to be expected.\r\n",
    "# However, MSE doesn't show a reversed pattern of recall. This proves that lower MSE doesn't necessarily mean a better model."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.1.1.6 Plot graph of recall scores split by model type and dataset range"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot recall scores. Use dodge parameter to create even space between bars when using hue.\r\n",
    "# Graph split by model type.\r\n",
    "sns.barplot(x=\"model_name\", y=\"recall_score\", data=mse_recall, hue=mse_recall[\"model_type\"], dodge=False)\r\n",
    "plt.xticks(rotation=30, horizontalalignment=\"right\", verticalalignment=\"top\")\r\n",
    "plt.ylabel(mse_recall[\"recall_score\"].name.capitalize())\r\n",
    "plt.title(\"recall of models split by type\".title())\r\n",
    "plt.legend(loc=\"upper right\")\r\n",
    "plt.show()\r\n",
    "# Graph shows that DRF and GBM have the similar results for the same dataset, except for beer full range dataset where DRF performed better.\r\n",
    "# Will use DRF models for predictions on the unseen dataset.\r\n",
    "\r\n",
    "# Graph split by dataset range.\r\n",
    "sns.barplot(x=\"model_name\", y=\"recall_score\", data=mse_recall, hue=mse_recall[\"dataset_range\"], dodge=False)\r\n",
    "plt.xticks(rotation=30, horizontalalignment=\"right\", verticalalignment=\"top\")\r\n",
    "plt.ylabel(mse_recall[\"recall_score\"].name.capitalize())\r\n",
    "plt.title(\"recall of models split by dataset range\".title())\r\n",
    "plt.legend(loc=\"upper right\")\r\n",
    "plt.show()\r\n",
    "# Graph shows that full range datasets generated better models than datasets with 2*std away ratings.\r\n",
    "# This was to be expected because the goal of the project is to predict outliers, not the bulk of average ratings.\r\n",
    "# Will keep outliers when testing the model on the unseen beer dataset."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1.2 Approved Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print MSE and recall dataframe.\r\n",
    "print(mse_recall)\r\n",
    "\r\n",
    "# Approved models (see discussion in Final Report):\r\n",
    "# alc_free_all_drf for unseen alcohol-free beer\r\n",
    "# light_all_drf for unseen light beer\r\n",
    "# regular_all_drf for unseen regular beer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 Review Process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create function to plot false negatives split by occurrence of observations in subgroups.\r\n",
    "\r\n",
    "def plot_occurrences(false_negatives_dictionary):\r\n",
    "    \"\"\"Subplots of false negatives split by occurrence in each dataset.\r\n",
    "\r\n",
    "    Args:\r\n",
    "    false_negatives_dictionary (dict): Dictionary of false negatives dataframes and their names.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    Seaborn countplot in subplots of false negatives.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # Create figure and set its title.\r\n",
    "    fig = plt.figure()\r\n",
    "    fig.suptitle(\"Occurrence of observations in false negatives\".title())\r\n",
    "    # Start index of subplots.\r\n",
    "    i=0\r\n",
    "\r\n",
    "    # Iterate through input dictionary.\r\n",
    "    for key, value in false_negatives_dictionary.items():\r\n",
    "        # Access name of dataframe.\r\n",
    "        name = str(key)\r\n",
    "\r\n",
    "        # Add subplots sequentially.\r\n",
    "        # Mark the first subplot as i+1 because subplot indices start at 1, and i is initialized at 0.\r\n",
    "        ax = fig.add_subplot(3, 4, i+1)\r\n",
    "        # Set title of subplot.\r\n",
    "        ax.set_title(f\"{name}\", fontsize=7, verticalalignment=\"top\", y=0.9)\r\n",
    "        \r\n",
    "        # Select rows of each subgroup.\r\n",
    "        too_few = value[value[\"occurrence\"] == \"too_few\"]\r\n",
    "        enough = value[value[\"occurrence\"] == \"enough\"]\r\n",
    "\r\n",
    "        # If both subgroups are present use whole dataframe for plotting, otherwise use the single series.\r\n",
    "        # This helps when aligning labels.\r\n",
    "        if len(too_few[\"occurrence\"]) > 0:\r\n",
    "            sns.countplot(x=\"occurrence\", data=value, order=value[\"occurrence\"].value_counts().index)\r\n",
    "        else:\r\n",
    "            sns.countplot(x=\"occurrence\", data=enough)\r\n",
    "\r\n",
    "        # Count bars to be plotted because it helps with setting bar width.\r\n",
    "        bar_number = value[\"occurrence\"].nunique()\r\n",
    "\r\n",
    "        # Set bar width. Matplotlib divides the plot area into bars according to the number of bars. \r\n",
    "        # The float parameter provided by user is not used as a constant.\r\n",
    "        # That's why setting a parameter dependent on number of bars cancels the division made by matplotlib.\r\n",
    "        # This results in bars with the same width.\r\n",
    "        for patch in ax.patches:\r\n",
    "            patch.set_width(0.3*bar_number)\r\n",
    "        # Show value counts of observations as labels on top of bars.\r\n",
    "        ax.bar_label(ax.containers[0])\r\n",
    "\r\n",
    "        # Set only y label to show they are counts.\r\n",
    "        # An x label would crowd the figure because of limited space between rows of graphs.\r\n",
    "        # The title of the whole figure already says what's on x axis.\r\n",
    "        ax.set(xlabel=\"\", ylabel=\"Count\")\r\n",
    "\r\n",
    "        # Calculate middle points of bars and use them to mark location of x-ticks.\r\n",
    "        midpoints = [patch.get_x() + patch.get_width() / 2 for patch in ax.patches]\r\n",
    "        ax.set_xticks(midpoints)\r\n",
    "        # Define list of labels and set them under x-ticks.\r\n",
    "        list_labels = list(value[\"occurrence\"].value_counts().index)\r\n",
    "        ax.set_xticklabels(list_labels, fontsize=7)\r\n",
    "\r\n",
    "        # Set common y limit for all subplots in order to have the same scale when visualizing them.\r\n",
    "        plt.ylim([0, 20])\r\n",
    "\r\n",
    "        # Increment the index for the next subplot.\r\n",
    "        i += 1\r\n",
    "    \r\n",
    "    # Show all graphs in one figure.\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "plot_occurrences(false_negatives)\r\n",
    "\r\n",
    "# Graphs show that most of the wrong predictions were generated for subgroups that had enough observations."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 6. DEPLOYMENT"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1. Plan Deployment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1.1 Import and clean unseen file following the same steps as with seen data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import unseen file.\r\n",
    "beer_unseen_raw = pd.read_csv(os.getcwd() + os.sep + \"unseen_data\" + os.sep + \"unseen_input\" + os.sep + \"beer_unseen_raw.csv\", header=0)\r\n",
    "\r\n",
    "# Standardize appeareance. Convert column labels to lowercase.\r\n",
    "beer_unseen_raw.columns = beer_unseen_raw.columns.str.lower()\r\n",
    "\r\n",
    "# Convert columns values to lowercase if they are strings.\r\n",
    "beer_unseen = beer_unseen_raw.applymap(lambda col:col.lower() if type(col) == str else col)\r\n",
    "\r\n",
    "# Convert Name column from object to string.\r\n",
    "beer_unseen[\"name\"] = beer_unseen[\"name\"].astype(\"string\")\r\n",
    "\r\n",
    "# Cut alcohol content from end of name and store as separate column.\r\n",
    "beer_unseen[\"abv\"] = [name.rsplit(maxsplit=1)[-1] for name in beer_unseen[\"name\"]]\r\n",
    "\r\n",
    "# Convert alcohol content to float.\r\n",
    "beer_unseen[\"abv\"] = beer_unseen[\"abv\"].astype(float)\r\n",
    "\r\n",
    "# Validation of uniqueness in beer names. Check for duplicates, remove if found.\r\n",
    "beer_unseen.drop_duplicates(subset=\"name\", keep=\"last\", inplace=True)\r\n",
    "\r\n",
    "# Create column for Pasteurization status.\r\n",
    "beer_unseen[\"pasteurization\"] = beer_unseen[\"name\"].str.contains(\"|\".join(unpasteurized_words))\r\n",
    "beer_unseen[\"pasteurization\"] = beer_unseen[\"pasteurization\"].replace({True: \"unpasteurized\", False: \"pasteurized\"})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1.2 Create datasets based on alcohol content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Split unseen dataset into alcohol-free, light and regular beer.\r\n",
    "alc_free_unseen = beer_unseen[beer_unseen[\"abv\"] <= 0.5]\r\n",
    "light_unseen = beer_unseen[(beer_unseen[\"abv\"] > 0.5) & (beer_unseen[\"abv\"] <= 3)]\r\n",
    "regular_unseen = beer_unseen[beer_unseen[\"abv\"] > 3]\r\n",
    "\r\n",
    "# Create list of unseen datasets, list of identificators and zip them.\r\n",
    "clean_unseen = [alc_free_unseen, light_unseen, regular_unseen]\r\n",
    "unseen_names = [\"alc_free_unseen\", \"light_unseen\", \"regular_unseen\"]\r\n",
    "unseen_dict = dict(zip(unseen_names, clean_unseen))\r\n",
    "\r\n",
    "# Export unseen datasets after cleaning.\r\n",
    "unseen_clean_path = os.getcwd() + os.sep + \"unseen_data\" + os.sep + \"unseen_output\" + os.sep + \"unseen_clean\"\r\n",
    "\r\n",
    "export_dict_as_csv(unseen_dict, unseen_clean_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1.3 Apply ML models on unseen data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Select unseen clean files.\r\n",
    "unseen_clean_files = glob.glob(unseen_clean_path + os.sep + \"*.csv\")\r\n",
    "\r\n",
    "# Import unseen datasets as dictionary into H2O.\r\n",
    "unseen_frames = files_to_h2o_frames(unseen_clean_files, \"rating\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write function to test models on unseen datasets.\r\n",
    "\r\n",
    "def test_model_on_unseen(frames, pred_output_folder):\r\n",
    "    \"\"\"Apply already trained model to an unseen dataset and output dictionary of dataset names and predictions.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "    frames (dict): Dictionary containing unseen frames.\r\n",
    "    pred_output_folder (str): Path to folder where to export predictions.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    csv file with predictions of each model.\r\n",
    "    dict: Dictionary of pandas dataframes containing predictions.\r\n",
    "    \r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # Create dictionary to store names of unseen datasets and their predictions.\r\n",
    "    unseen_prediction_dict = {}\r\n",
    "\r\n",
    "    # Iterate through dictionary and store its elements under short variable names to help with readability.\r\n",
    "    for key, value in frames.items():\r\n",
    "        # Access the name.\r\n",
    "        unseen_name = str(key) + \"_pred\"\r\n",
    "\r\n",
    "        # Access the frame.\r\n",
    "        unseen_frame = frames[key][\"frame\"]\r\n",
    "\r\n",
    "        # Import models.\r\n",
    "        if \"alc_free\" in unseen_name:\r\n",
    "            imported_model = h2o.import_mojo(os.getcwd() + os.sep + \"beer_output\" + os.sep + \"models\" + os.sep + \"drf_models\" + os.sep + \"alc_free_all_drf.zip\")\r\n",
    "\r\n",
    "        elif \"light\" in unseen_name:\r\n",
    "            imported_model = h2o.import_mojo(os.getcwd() + os.sep + \"beer_output\" + os.sep + \"models\" + os.sep + \"drf_models\" + os.sep + \"light_all_drf.zip\")\r\n",
    "\r\n",
    "        elif \"regular\" in unseen_name:\r\n",
    "            imported_model = h2o.import_mojo(os.getcwd() + os.sep + \"beer_output\" + os.sep + \"models\" + os.sep + \"drf_models\" + os.sep + \"regular_all_drf.zip\")\r\n",
    "\r\n",
    "        else:\r\n",
    "            imported_model = h2o.import_mojo(os.getcwd() + os.sep + \"beer_output\" + os.sep + \"models\" + os.sep + \"drf_models\" + os.sep + \"beer_all_drf.zip\")\r\n",
    "        \r\n",
    "        # Generate predictions.\r\n",
    "        unseen_prediction = imported_model.predict(unseen_frame)\r\n",
    "\r\n",
    "        # Frames plus predictions.\r\n",
    "        unseen_plus_pred = unseen_frame.cbind(unseen_prediction)\r\n",
    "\r\n",
    "        # Convert frames to dataframes.\r\n",
    "        unseen_pred_df = unseen_plus_pred.as_data_frame()\r\n",
    "\r\n",
    "        # Append dictionary with prediction.\r\n",
    "        unseen_prediction_dict[unseen_name] = unseen_pred_df\r\n",
    "\r\n",
    "        # Export predictions dataframes.\r\n",
    "        unseen_pred_df.to_csv(pred_output_folder+ os.sep + unseen_name + \".csv\", index=False)\r\n",
    "\r\n",
    "    return unseen_prediction_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1.4 Generate and export predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Path to store predictions from unseen datasets.\r\n",
    "unseen_predictions_path = os.getcwd() + os.sep + \"unseen_data\" + os.sep + \"unseen_output\" + os.sep + \"unseen_predictions\"\r\n",
    "\r\n",
    "# Call function to generate and export predictions on unseen datasets.\r\n",
    "unseen_predictions = test_model_on_unseen(unseen_frames, unseen_predictions_path)\r\n",
    "\r\n",
    "# In alc_free and light there is a Warning:\r\n",
    "# Test/Validation column â€œStyleâ€ has levels not trained on alt and â€œFlavorâ€ has levels not trained on herb.\r\n",
    "# This means that the training frame the model was built upon had no alt style and no herb flavor."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1.5 Select and export false negatives"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Select false negatives from unseen datasets.\r\n",
    "unseen_false_negatives = select_false_negatives(unseen_predictions, \"rating\", \"predict\", 5)\r\n",
    "\r\n",
    "# Path to store false negatives from unseen datasets.\r\n",
    "unseen_false_negatives_path = os.getcwd() + os.sep + \"unseen_data\" + os.sep + \"unseen_output\" + os.sep + \"unseen_false_negatives\"\r\n",
    "\r\n",
    "# Export false negatives from unseen datasets.\r\n",
    "export_dict_as_csv(unseen_false_negatives, unseen_false_negatives_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1.6 Calculate and export recall scores on unseen datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate recall score of models on unseen datasets.\r\n",
    "unseen_recall = calculate_recall_score(unseen_predictions, \"rating\", \"predict\", 5)\r\n",
    "\r\n",
    "# Sort unseen_recall by recall score.\r\n",
    "unseen_recall.sort_values(\"recall_score\", ascending=False, inplace=True)\r\n",
    "\r\n",
    "# Path where to store recall of models on unseen datasets.\r\n",
    "unseen_recall_path = os.getcwd() + os.sep + \"unseen_data\" + os.sep + \"unseen_output\" + os.sep + \"unseen_metrics\" + os.sep + \"recall_unseen.csv\"\r\n",
    "\r\n",
    "# Export recall score of models on unseen datasets.\r\n",
    "unseen_recall.to_csv(unseen_recall_path, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1.7 Evaluate results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print recall dataframe.\r\n",
    "print(unseen_recall)\r\n",
    "\r\n",
    "# Plot recall scores on unseen datasets.\r\n",
    "ax = sns.barplot(x=\"model_name\", y=\"recall_score\", data=unseen_recall)\r\n",
    "ax.set(xlabel=\"Model name\", ylabel=\"Recall\", title=\"recall of predictions in unseen datasets\".title())\r\n",
    "ax.set_ylim([0, 1])\r\n",
    "ax.bar_label(ax.containers[0])\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# Graph shows that regular beer is the only one from which low rated beer was detected.\r\n",
    "# Alcohol-free and light beers either had a scoze of 0, or didn't have low rated beer in the dataset.\r\n",
    "# It's possible to improve these scores or lack of score by collecting more data."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print total number of rows and ratings lower than 5 in each unseen dataset to bring context for recall score.\r\n",
    "\r\n",
    "for key, value in unseen_predictions.items():\r\n",
    "    # Check number of total rows.\r\n",
    "    total_rows = value.shape[0]\r\n",
    "    # Check number of low ratings.\r\n",
    "    lower_than_five = value[value[\"rating\"] < 5]\r\n",
    "    only_lows = lower_than_five.shape[0]\r\n",
    "    # Check number of detected low ratings.\r\n",
    "    true_positives = value[(value[\"rating\"] < 5) & (value[\"predict\"] < 5)]\r\n",
    "    detected = true_positives.shape[0]\r\n",
    "    # Check number of undetected low ratings.\r\n",
    "    false_negatives = value[(value[\"rating\"] < 5) & (value[\"predict\"] >= 5)]\r\n",
    "    undetected = false_negatives.shape[0]\r\n",
    "    # Print all counts from above.\r\n",
    "    print(f\"Dataset: {key} \\nRows: {total_rows} \\nNumber of low ratings: {only_lows}, out of which: \\nDetected: {detected} \\nUndetected: {undetected}\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}